**What is intelligence?**

The ability of an agent to represent the surrounding reality in roder to improve its succes there.

**Successes of AI**

Neural networks → magnificent achievements
- classification (dog breeds, faces, birdsong, flowers, etc.)
- text generation (chatGPT, Bing, DeepSeek, etc.)
- image generation (midjourney, Dall-E, Dreamstudio, etc.)
- autonomous cars / AI driving assistants
- etc.
- Billion USD business (2023: ~ 200 bUSD)

**General aspects of AI**

- Coding approaches -- can a machine be smarter then its creator?
    - traditional algorithms
        - we understand the problem
        - write a code
        - machine does the same than we sould do, just faster
        - machine is not smarter (just faster)
    - artificial intelligence
        - we understand learning
        - write a learning code
        - the code learns the problem and solves it
        - we do no know the solution of the problem (only the learning strategy)
        - machine is smarter in spcific fields

- challenges
    - no "I don't know" category (needs balanced datasets)
    - lack of generalization ("catastrophic forgetting")
    - no error self-control
        - adversarial attack
        - hallucinations
    - no goals, no planning

- approaches to problem solving


    |          | observation | modelling | computation | action |
    |----------|:-----------:|:---------:|:-----------:|:------:|
    | historic times | human | human     | human       | human  |
    | machines | machine     | human     | human       | human  |
    | computers | machine    | human     | machine     | human  |
    | AI       | machine     | machine   | machine     | human  |

**Representation of the world**

- Aspects of reality
    - general reality: laws and generic algorithms describing how the world works
        - examples
            - map describing the streets
            - LLM body, to find out which token comes
            - sport example
                - tool of the athlete (bycicle, gun, etc), - the physical ability (strength, speed, precision)
                - to improve: better tools, hard training
        - in AI: part of the (trained) hardware → represented by millions to ~100billions of parameters
    - actual reality: describes the actual environment, the objects, persons and phenomena that are actually active
        -examples
            - in the map the actual traffic, road works, detours, obstacles, etc.
            - LLMs: the prompt describing the actual task
            - in sport: actual form of the athlete
        - in AI: part of the input → ~million tokens
    - in main stream AI general reality is represented much deeper
    - importance:
        - general reality: the ability to be able to describe the world
        - actual reality: the actual task to be solved
    - initial conditions
        - represent actual reality
        - in science (differential equations): exact representation of future
        - in real systems (noisy, chaotic): represents reality for a range
        - there is always a validity range
            - deviation from the "exact" path in simulations
            - hallucination radius in LLMs
    - maintain precision:
        - needs always project the predictions to the actual reality
        - control can be simple (even linear)
        - example: lossless compression

- Worldview: science vs intelligence
    - alma

- Characterization of reality
    - gather information
        - facts: all possible measurement can be performed in reality → infinitely many
        - interpretation agnostic
        - in IT
            - big data approach, collect all available information (databases, data lakes)
            - avoid information loss
            - very large amount of data
        - example: pixel color, logs, amplitudes in a wave
    - contexts
        - segments (fixed points) of the world
            - we have a finite set of "sensible" measurements → features
            - we have many (infinitely) possible facts
            - for each measurement value there belongs a lot of facts → equivalence classes (contexts)
            - relevant features: class constant functions, irrelevant features: remainder
            - any context can be characterized in this way
        - how to define classes?
            - we want some classification (supervised)
            - natural relation (e.g. temporal subsequence)
            - causal relations
            - symmetries: classes remain invariant under symmetry transformaiton
        - how can we find the appropriate features?
            - features should be functions of the classes
            - mathematically: measure/probability theory
                - sigma algebra generated by the classes
                - features are measurable functions, or random variables
            - practically: a feature should depend on the class only → constant over the class
            $$f(x)=f_C \;\rm{  if  }\; x \in C \;\Rightarrow\; f(C)$$
        - features depend on the classes, ie. on the context
            - no "general reality" on the level of features
            - context change: change of relevant features (renormalization)
    - examples of class-constant functions as relevant features
        - "objects": solid states
            - relevant features: shape descriptors
            - irrelevant features: place, time, rotation
        - "angle": between two lines
            - relevant feature: angle
            - irrelevant: position, rotation
        - natural laws: persistent relation between measurable quantities
            - Newton law: $F-ma=0$
            - energy conservation
            - market model: $F(\dot S, S,\dots)=0$ is invariant -- approximate laws, c.f. intelligence and science

- Choosing coordinates
    - coordination: objects (classes) – measurable functions (features, properties, context) – values
    - coordination: objects – coordinates – values
    - general (fixed) coordinates
        - no need to name the properties → $p:(x_1,x_2,\dots x_d)$
        - mathematical spaces, coordinate system
        - word embedding
            - one-hot envcoding
            - vector embedding: ~thousand dimensional space
    - optimal coordination (coding)
        - hierarchical structure: new phenomena for composite classes (like four-legged)
        - coordinates characteristic to the object (context)
        - we need to name the features (coordinates) $$ \rm{object} : ("feature"_1:x_1,"feature"_2:x_2, \dots)$$
        - Huffmann coding: back to front strategy, group the least probable events
        - Shannon coding: front to back, groups events halving the total probability
        - logarithmic convergence (c.f. twenty questions)
        - object dependent coordination
    - graph databases → Resource Description Framework
        - lot of possible solutions
        - triplet representation of reality
        - knowledge graph representation
        - each entry is a triplet: 

        ```
        (subject, predicate, object) → (name, property, value)
        ```

        - example:  `Dorothy : son = Bill` 
        - own development: TriosDB → lightweight RDF solution
        - values are vectors
        - circular representation for efficient queries

            `Bill : Dorothy = son,  son : Bill = Dorothy`
        - context as data modules

- Representation strategies
    - Action focused (System 1)
        - balanced datasets
        - specialized, catstrophic forgetting
        - no control, hallucinations, error prone
    - Representation focused (System 2)

- Processing continuous input
    - goal: discrete features from continuous input
    - context
        - from labels or from temporal subsequence
        - same time series can belong to different context
        - scale as a natural context
    - features: 
        - class constant functions → time constant (conserved) quantities
        - necessary conditions to belong to a class
    - we need a parametrizable functional space (e.g. DNN, LLT, ...)

- Time series analysis
    - linear models collection (LLT)

- DNN/CNN models

- Retrieval Augmented Generation (RAG) models

- relational databases

- non-relational databases
    - Mongo

- LLT: linear law analysis for time series
    - a single linear law
        - embed $n$ data from $m$ subsequent starting point from a time series with a certain time step $$X = \{ X_{ab} = x(t_0+(a+b)\Delta t) \;\vert\; a=1,\dots,m; b=1,\dots n\}$$
        - determine that linear map that brings all elements to a minimal length vector $$Xw=\xi,\; \mathrm{where}\; \xi\;\mathrm{is\ minimal\ and} \;w^2=1.$$
        - extremum condition $$ w^T X^TX w -\lambda w^Tw = \mathrm{minimal}\quad \Rightarrow \quad X^TXw=\lambda w$$
        - minimum $$\xi^2 = w^T X^TX w = \lambda w^Tw = \lambda,$$
        therefore we seek the minimal eigenvalue
    - a collection of linear laws
        - a class may consist of several linear laws
        - in practice: collect all laws, where any of them is true for an element of a class
        - if rules do not fit → “I don’t know” category (outlier)
        - several scales (context exploration)
    - classification strategy
        - try all laws for the new sample
        - keep the top 5% (to avoid false laws due to corrupted database)
        - use the result as new features → near zero for the correct class
        - if no near zero result → outlier
        - standard classifiers for get final result (SVM, KNN, RF)
        - benchmarking: publicly available datasets (paper: "Adaptive law-based feature representation for time series classification", 
        MT Kurbucz, B Hajós, BP Halmos, VÁ Molnár, A Jakovác, Scientific Reports 15 (1), 41775)
        - applications:
            - mechanical motions
            - ECG signal procesing
            - bitcoin price prediction
